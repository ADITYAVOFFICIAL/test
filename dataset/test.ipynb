{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 103\u001b[0m\n\u001b[1;32m     88\u001b[0m data_transforms \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m: transforms\u001b[38;5;241m.\u001b[39mCompose([\n\u001b[1;32m     90\u001b[0m         transforms\u001b[38;5;241m.\u001b[39mResize((\u001b[38;5;241m224\u001b[39m, \u001b[38;5;241m224\u001b[39m)),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     99\u001b[0m     ]),\n\u001b[1;32m    100\u001b[0m }\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Assuming train.csv is loaded and contains `image_link` and `entity_value` columns\u001b[39;00m\n\u001b[0;32m--> 103\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m CustomDataset(image_links\u001b[38;5;241m=\u001b[39m\u001b[43mtrain_data\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_link\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    104\u001b[0m                               entity_values\u001b[38;5;241m=\u001b[39mtrain_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity_value\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    105\u001b[0m                               transform\u001b[38;5;241m=\u001b[39mdata_transforms[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    106\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m CustomDataset(image_links\u001b[38;5;241m=\u001b[39mval_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_link\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    107\u001b[0m                             entity_values\u001b[38;5;241m=\u001b[39mval_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentity_value\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    108\u001b[0m                             transform\u001b[38;5;241m=\u001b[39mdata_transforms[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m    110\u001b[0m dataloaders \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    111\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m: DataLoader(train_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m),\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m'\u001b[39m: DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    113\u001b[0m }\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import pandas as pd\n",
    "train_data = pd.read_csv('train.csv')\n",
    "val_data = pd.read_csv('val.csv')\n",
    "\n",
    "# Step 3: Define a custom dataset class (assuming entity value is continuous)\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, image_links, entity_values, transform=None):\n",
    "        self.image_links = image_links\n",
    "        self.entity_values = entity_values\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_links)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image = Image.open(self.image_links[idx])\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        label = torch.tensor(float(self.entity_values[idx]))  # Assuming continuous target\n",
    "        return image, label\n",
    "\n",
    "# Step 4: Define the model\n",
    "class EfficientNetModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EfficientNetModel, self).__init__()\n",
    "        # Load a pretrained EfficientNet model\n",
    "        self.efficientnet = EfficientNet.from_pretrained('efficientnet-b0')\n",
    "        \n",
    "        # Adjust the final layer to match the output size (1 for entity value prediction)\n",
    "        self.fc = nn.Linear(self.efficientnet._fc.in_features, 1)\n",
    "        self.efficientnet._fc = nn.Identity()  # Remove original FC layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.efficientnet(x)\n",
    "        output = self.fc(features)\n",
    "        return output\n",
    "\n",
    "# Step 5: Set up training parameters\n",
    "def train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n",
    "    model = model.to(device)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs-1}')\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "\n",
    "            # Iterate over data\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # Zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # Forward\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
    "\n",
    "            print(f'{phase} Loss: {epoch_loss:.4f}')\n",
    "\n",
    "    return model\n",
    "\n",
    "# Step 6: Prepare dataset and data loader\n",
    "# Define the image transformations\n",
    "data_transforms = {\n",
    "    'train': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "    'val': transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    ]),\n",
    "}\n",
    "\n",
    "# Assuming train.csv is loaded and contains `image_link` and `entity_value` columns\n",
    "train_dataset = CustomDataset(image_links=train_data['image_link'],\n",
    "                              entity_values=train_data['entity_value'],\n",
    "                              transform=data_transforms['train'])\n",
    "val_dataset = CustomDataset(image_links=val_data['image_link'],\n",
    "                            entity_values=val_data['entity_value'],\n",
    "                            transform=data_transforms['val'])\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(train_dataset, batch_size=32, shuffle=True),\n",
    "    'val': DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "}\n",
    "\n",
    "# Step 7: Initialize model, loss, and optimizer\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = EfficientNetModel()\n",
    "criterion = nn.MSELoss()  # Mean Squared Error Loss for continuous values\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Step 8: Train the model\n",
    "model = train_model(model, dataloaders, criterion, optimizer, num_epochs=10)\n",
    "\n",
    "# Step 9: Save the model\n",
    "torch.save(model.state_dict(), 'efficientnet_entity_value_model.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
